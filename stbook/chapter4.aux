\relax 
\newlabel{part2}{{III}{57}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Robust linear regression}{57}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:robreg}{{4}{57}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}The linear regression model}{57}}
\newlabel{eq:linear_regr_model}{{4.1}{57}}
\newlabel{eq:linear_regr_model_sample}{{4.2}{57}}
\newlabel{eq:linear_regr_model_sample_bis}{{4.3}{57}}
\newlabel{eq:location_scale_regr_model}{{4.4}{58}}
\newlabel{eq:distr_function_yi}{{4.5}{58}}
\newlabel{eq:dens_function_yi}{{4.6}{58}}
\newlabel{eq:location_scale_model}{{4.7}{58}}
\citation{rousseeuw:leroy:1987}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Different types of outliers}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Vertical outlier, good leverage point and bad leverage point}}{60}}
\newlabel{fig:outlier_types}{{4.1}{60}}
\citation{maronna:etal:2006}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}LS estimation}{61}}
\newlabel{eq:LS_min}{{4.8}{61}}
\newlabel{eq:LS_equations}{{4.9}{61}}
\newlabel{eq:LS_estimator}{{4.10}{61}}
\citation{edgeworth:1887}
\citation{huber:1981}
\newlabel{eq:regression_equivariance}{{4.11}{62}}
\newlabel{eq:scale_equivariance}{{4.12}{62}}
\newlabel{eq:affine_equivariance}{{4.13}{62}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}M estimation}{62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}L\textsubscript  {1} or Least Absolute Deviation (LAD) estimation}{62}}
\newlabel{eq:L1_min}{{4.14}{62}}
\newlabel{eq:L1_equations}{{4.15}{62}}
\citation{huber64}
\citation{huber64}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}The principle of M estimation}{63}}
\newlabel{eq:M_min}{{4.16}{63}}
\citation{maronna:etal:2006}
\newlabel{eq:Tukey_Biweight_function}{{4.17}{64}}
\newlabel{eq:M_equations}{{4.18}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Huber loss function $\rho _{\kappa }^{\textsc  {\lowercase {H}}}$ and score function $\psi _{\kappa }^{\textsc  {\lowercase {H}}}$}}{65}}
\newlabel{fig:rho_psi_Huber}{{4.2}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Tukey-Biweight loss function $\rho _{\kappa }^{\textsc  {\lowercase {B}}}$ and score function $\psi _{\kappa }^{\textsc  {\lowercase {B}}}$}}{65}}
\newlabel{fig:rho_psi_Biweight}{{4.3}{65}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}M estimation as a generalization of maximum likelihood (ML) estimation}{65}}
\newlabel{eq:ML_beta_sigma_min}{{4.19}{66}}
\newlabel{eq:ML_beta_min}{{4.20}{66}}
\newlabel{eq:ML_beta_sigma_equations}{{4.21}{66}}
\citation{maronna:etal:2006}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Practical implementation of M estimates}{67}}
\newlabel{subsec:practical_implementation_Mestimate}{{4.4.4}{67}}
\newlabel{eq:weighted_LS_equations}{{4.22}{67}}
\@writefile{toc}{\contentsline {subsubsection}{Regression M estimate with preliminary scale estimation}{67}}
\citation{Koenker:1978}
\citation{Koenker:2005}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.5}Regression quantiles as regression M estimates}{68}}
\newlabel{eq:quantile_regr_min}{{4.23}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.6}Monotone vs. redescending M estimators}{68}}
\citation{Mallows:1975}
\citation{rousseeuw:leroy:1987}
\citation{Ronchetti:Rousseeuw:1985}
\citation{Maronna:1979}
\citation{maronna:etal:2006}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.7}GM estimation}{69}}
\newlabel{eq:GM_equations_1}{{4.24}{69}}
\newlabel{eq:GM_equations_2}{{4.25}{69}}
\newlabel{eq:leverage}{{4.26}{69}}
\citation{Theil:1950}
\citation{Brown:1951}
\citation{Sen:1968}
\citation{Jaeckel:1972}
\citation{Andrews:1974}
\citation{Siegel:1982}
\citation{Rousseeuw:1983}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Robust regression with a high breakdown point}{70}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}LTS and LMS estimation}{70}}
\citation{rousseeuw&vdriessen99}
\citation{rousseeuw:1984}
\citation{rousseeuw:yohai:1984}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}S estimation}{71}}
\citation{rousseeuw:yohai:1984}
\citation{rousseeuw:yohai:1984}
\newlabel{eq:M_scale_equation}{{4.27}{72}}
\newlabel{eq:M_scale_equation_res}{{4.28}{72}}
\citation{Hossjer:1992}
\citation{yohai:1987}
\citation{Yohai:1988}
\citation{Mendes:1996}
\citation{Gervini:2002}
\newlabel{eq:S_Minequality_1}{{4.29}{73}}
\citation{yohai:1987}
\citation{yohai:1987}
\citation{maronna:etal:2006}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}MM estimation}{74}}
\newlabel{subsec:MM_estimation}{{4.5.3}{74}}
\newlabel{eq:MM_min}{{4.30}{74}}
\newlabel{eq:MM_inequality}{{4.31}{74}}
\newlabel{eq:MM_equations}{{4.32}{74}}
\citation{maronna:etal:2006}
\citation{salibian:yohai:2006}
\citation{rousseeuw:1984}
\@writefile{toc}{\contentsline {subsubsection}{Numerical computation of the S- and MM-estimate}{75}}
\newlabel{eq:S_min}{{4.33}{75}}
\citation{Salibian-Barrera:2004}
\newlabel{eq:N}{{4.34}{76}}
\newlabel{eq:S_algorithm}{{4.35}{76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}MS-estimation}{77}}
\newlabel{eq:linear_regr_model_MS}{{4.36}{77}}
\citation{maronna:etal:2006}
\citation{Croux:2003}
\citation{Croux:2003}
\citation{Hansen:1982}
\citation{Croux:2003}
\citation{Croux:2003}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Robust inference for M-, S- and MM-estimators}{78}}
\newlabel{sec:inference}{{4.6}{78}}
\citation{Croux:2003}
\citation{Croux:2003}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Asymptotic distribution of M-, S- and MM-estimators}{79}}
\newlabel{subsec:asymptotic_distr_M_S_MM_estimators}{{4.6.1}{79}}
\citation{Croux:2003}
\newlabel{eq:V_MM}{{4.37}{80}}
\newlabel{eq:G_MM}{{4.38}{80}}
\newlabel{eq:Omega_MM}{{4.39}{80}}
\newlabel{eq:Avar_betahat_MM}{{4.40}{80}}
\newlabel{eq:Avar_betahat_S}{{4.41}{80}}
\newlabel{eq:Acov_betahat_MM_S}{{4.42}{80}}
\citation{Croux:2003}
\newlabel{eq:A}{{4.43}{81}}
\newlabel{eq:a}{{4.44}{81}}
\newlabel{eq:A_S}{{4.45}{81}}
\newlabel{eq:a_S}{{4.46}{81}}
\citation{Croux:2003}
\newlabel{eq:Avar_betahat_LS}{{4.47}{82}}
\newlabel{eq:A_a_LS}{{4.48}{82}}
\newlabel{eq:Acov_betahat_LS_S}{{4.49}{82}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Robust confidence intervals and tests with robust regression estimators}{83}}
\@writefile{toc}{\contentsline {subsubsection}{Inference for a single linear combination of the regression parameters}{83}}
\@writefile{toc}{\contentsline {subsubsection}{Inference for several linear combinations of the regression parameters}{84}}
\citation{Greene:1997}
\citation{Anderson:1984}
\citation{Renaud:2010}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}Robust $R^{2}$}{85}}
\newlabel{eq:R2_classic_SS}{{4.50}{85}}
\newlabel{eq:R2_classic_corr}{{4.51}{85}}
\newlabel{eq:phi2}{{4.52}{85}}
\newlabel{eq:R2_adj}{{4.53}{85}}
\citation{maronna:etal:2006}
\citation{croux:dehon:2003}
\newlabel{eq:R2_rho}{{4.54}{86}}
\newlabel{eq:R2_S}{{4.55}{86}}
\citation{Renaud:2010}
\citation{Renaud:2010}
\newlabel{eq:R2_w}{{4.56}{87}}
\newlabel{eq:Rtilde2_w}{{4.57}{87}}
\newlabel{eq:Rtilde2_w_a}{{4.58}{87}}
\citation{Hausman:1978}
\newlabel{eq:Rtilde2_w_a_adj}{{4.59}{88}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.4}Extension of the Hausman test to check for the presence of outliers}{88}}
\newlabel{subsec:Hausman}{{4.6.4}{88}}
\citation{Dehon:2012}
\@writefile{toc}{\contentsline {subsubsection}{Some preliminary results}{89}}
\newlabel{eq:Hausman_sigmahat_S-MM}{{4.60}{89}}
\citation{Omelka:2010}
\newlabel{eq:Hausman_normality_S-MM}{{4.61}{90}}
\newlabel{eq:Hausman_sigmahat_S-LS}{{4.62}{90}}
\newlabel{eq:Hausman_normality_S-LS}{{4.63}{90}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison of LS and S}{90}}
\newlabel{Hausman_SvsLS}{{4.64}{90}}
\citation{maronna:etal:2006}
\citation{Omelka:2010}
\@writefile{toc}{\contentsline {subsubsection}{Comparison of S and MM}{91}}
\newlabel{Hausman_MMvsS}{{4.65}{91}}
\citation{rousseeuw:leroy:1987}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Examples}{92}}
\@writefile{toc}{\contentsline {subsubsection}{Comparing estimators}{92}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces \leavevmode {\color  {red}Caption needed}}}{92}}
\newlabel{fig:stars_scatterplot}{{4.4}{92}}
\@writefile{toc}{\contentsline {subsubsection}{LTS}{93}}
\@writefile{toc}{\contentsline {subsubsection}{LMS}{94}}
\@writefile{toc}{\contentsline {subsubsection}{M-estimator}{94}}
\@writefile{toc}{\contentsline {subsubsection}{GM-estimator}{94}}
\@writefile{toc}{\contentsline {subsubsection}{S-estimator}{95}}
\@writefile{toc}{\contentsline {subsubsection}{MM-estimator}{96}}
\@writefile{toc}{\contentsline {subsubsection}{Identifying outliers}{96}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Appendix 1: M-estimators of location and scale}{101}}
\newlabel{sec:robreg:appendix1}{{4.8}{101}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.1}M-estimator of location}{101}}
\newlabel{eq:M_location_equation}{{4.66}{101}}
\citation{Huber:2009}
\citation{maronna:etal:2006}
\citation{Huber:2009}
\citation{maronna:etal:2006}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.2}M-estimator of scale}{102}}
\newlabel{eq:M_scale_loc_equation}{{4.67}{102}}
\citation{Croux:2003}
\citation{Hansen:1982}
\@writefile{toc}{\contentsline {section}{\numberline {4.9}Appendix 2: Generalized Method of Moments (GMM) and asymptotic distributions of regression M-, S- and MM-estimators}{103}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.1}GMM-estimation principle}{103}}
\newlabel{Eq:GMM_moments_conditions}{{4.68}{103}}
\newlabel{Eq:GMM_equations}{{4.69}{103}}
\newlabel{Eq:GMM_estimator_normality}{{4.70}{104}}
\newlabel{Eq:GMM_estimator_V}{{4.71}{104}}
\newlabel{Eq:GMM_estimator_G_Omega}{{4.72}{104}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.2}M-, S- and MM-estimators as GMM-estimators}{104}}
\newlabel{Eq:GMM_equations_M}{{4.73}{104}}
\newlabel{Eq:GMM_moment_function_M}{{4.74}{104}}
\newlabel{eq:S_min_rho0}{{4.75}{104}}
\citation{rousseeuw:yohai:1984}
\newlabel{Eq:GMM_equations_S}{{4.76}{105}}
\newlabel{Eq:GMM_equations_MM}{{4.77}{105}}
\newlabel{Eq:GMM_moment_function_MM}{{4.78}{106}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.3}Asymptotic variance matrix of an MM-estimator}{106}}
\@writefile{toc}{\contentsline {subsubsection}{If the observations $\mathopen {}\mathclose \bgroup \originalleft ( \mathbf  x_{i},y_{i}\aftergroup \egroup \originalright ) $, $i = 1, \dots  , n$, are generated by a stationary and ergodic process, and are independent (Assumption A1)}{106}}
\citation{Croux:2003}
\citation{Croux:2003}
\@writefile{toc}{\contentsline {subsubsection}{In absence of heteroskedasticity (Assumption A2)}{108}}
\@writefile{toc}{\contentsline {subsubsection}{If the distribution of the error terms is symmetric around zero (Assumption A3)}{108}}
\citation{yohai:1987}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.4}Asymptotic variance matrix of an S-estimator}{109}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.5}Asymptotic variance matrix of an M-estimator}{109}}
\@setckpt{chapter4}{
\setcounter{page}{111}
\setcounter{equation}{78}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{25}
\setcounter{mpfootnote}{0}
\setcounter{part}{3}
\setcounter{chapter}{4}
\setcounter{section}{9}
\setcounter{subsection}{5}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{nprt@mantissa@digitsbefore}{0}
\setcounter{nprt@mantissa@digitsafter}{0}
\setcounter{nprt@exponent@digitsbefore}{0}
\setcounter{nprt@exponent@digitsafter}{0}
\setcounter{nprt@digitsfirstblock}{1}
\setcounter{nprt@blockcnt}{1}
\setcounter{nprt@cntprint}{0}
\setcounter{todo}{34}
\setcounter{NAT@ctr}{0}
}
